<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="emk" />
  <meta name="dcterms.date" content="2025/06/05" />
  
  
  <title>Setting the stage: objects &amp; notation</title>
  

  
  <!-- CSS -->
    <link rel="stylesheet" href="math-blog.css" />
  
  <!-- MathJax (only if --mathjax flag is used) -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
  
  <!-- Analytics or other header includes -->
  </head>

<body>
  <!-- Title -->
    <header class="title-header">
    <h1 class="title">Setting the stage: objects &amp; notation</h1>
  </header>
  
  <!-- Navigation -->
  <nav class="post-nav">
    <a href="../index.html">← Back to Homepage</a>
        | <a href="blog-index.html">← Back to Blog Index</a>
      </nav>

  <!-- Main content -->
  <main>
    <p>Let <span class="math inline">\(\mathcal{X}\times
    \mathcal{Y}\)</span> be the data space split along an input-label
    axis. The hypothesis class is a collection of functions <span
    class="math inline">\(f \in \mathcal{F}\)</span> <span
    class="math display">\[f : \mathcal{X}\times \Theta \to
    \mathcal{Y}.\]</span> For example, the hypothesis class could be a
    neural network with <span class="math inline">\(P\)</span> weights
    (and biases), then <span class="math inline">\(\Theta =
    \mathbb{R}^P\)</span> and <span
    class="math inline">\(f(\mathbf{\boldsymbol{x}},
    \mathbf{\boldsymbol{\theta}})\)</span> would be the function defined
    by the network.</p>
    <p>A <em>solution</em> to the supervised learning problem could be a
    <span class="math inline">\(\mathbf{\boldsymbol{\theta}}^*\)</span>
    (a set of parameters) such that <span
    class="math display">\[f(\mathbf{\boldsymbol{x}}_i,
    \mathbf{\boldsymbol{\theta}}^*) \approx y_i\]</span> for all given
    (training) data points <span
    class="math inline">\(\{(\mathbf{\boldsymbol{x}}_i, y_i)\}_{i = 1}^N
    \subseteq \mathcal{X}\times \mathcal{Y}\)</span>. The whole of
    machine learning enterprise rests on the assumption, or rather
    observation that <span class="math inline">\(f\)</span> also
    predicts the label for yet unseen data; i.e. <span
    class="math inline">\(f(\mathbf{\boldsymbol{x}}_{\text{new}},
    \mathbf{\boldsymbol{\theta}}^*) \approx y_{\text{new}}\)</span> for
    yet unseen data <span
    class="math inline">\((\mathbf{\boldsymbol{x}}_{\text{new}},
    y_{\text{new}})\)</span> assumed to come from the same distribution
    as the training data in <span
    class="math inline">\(\mathcal{X}\times \mathcal{Y}\)</span>.
    Whether this is possible heavily depends on the hypothesis class
    <span class="math inline">\(\mathcal{F}\)</span>. Those functions
    describable by a neural network and easily discoverable by the
    common gradient based algorithms have proven to work well for
    distributions of data we have collected and thrown at these machine
    learning models.</p>
    <p>Finding this <span
    class="math inline">\(\mathbf{\boldsymbol{\theta}}^*\)</span> is
    typically achieved by a variant of gradient descent on the loss
    function defined of the form <span
    class="math display">\[\label{eq:lossfn}
        \ell(\mathbf{\boldsymbol{\theta)}} = \frac{1}{N} \sum_{i = 1}^N
    \ell_i(\mathbf{\boldsymbol{\theta}}) +
    R(\mathbf{\boldsymbol{\theta}}) \quad \text{ with } \quad
    \ell_i(\mathbf{\boldsymbol{\theta}}) =
    c(f(\mathbf{\boldsymbol{x}}_i, \mathbf{\boldsymbol{\theta}}),
    y_i),\]</span> where <span class="math inline">\(R :\Theta \to
    \mathbb{R}_{\geq 0 }\)</span> is is called the regularizer,
    responsible for biasing the hypothesis space to simpler functions
    (usually meaning smaller norm parameters), and <span
    class="math inline">\(c : \mathcal{Y}\times \mathcal{Y}\to
    \mathbb{R}_{\geq 0}\)</span> is called the cost function, measuring
    how far a prediction <span class="math inline">\(\hat y\)</span> is
    from <span class="math inline">\(y\)</span> with <span
    class="math inline">\(c(\hat y, y)\)</span> (usually <span
    class="math inline">\(c(y, y) = 0\)</span>).</p>
    <ul>
    <li><p>For classification problems (with <span
    class="math inline">\(K\)</span> classes) it is common to take <span
    class="math inline">\(\mathcal{Y} =  \Delta^K = \{(p_1, \ldots, p_K)
    \in \mathbb{R}^K : p_i \geq 0, \sum_{i = 1}^K p_i = 1\}\)</span> the
    probability <span class="math inline">\(K\)</span>-simplex, and the
    cost function as the cross entropy loss <span
    class="math inline">\(c(\hat y, y) = - \sum_{i = 1}^k y_i \log \hat
    y_i\)</span>.</p></li>
    <li><p>It is possible to consider <span class="math inline">\(c :
    \widehat{\mathcal{Y}} \times \mathcal{Y}\to \mathbb{R}_{\geq
    0}\)</span>, for example when the labels <span
    class="math inline">\(y_i\)</span> were given as indices and not
    one-hot vectors. But unless there is a correspondence between <span
    class="math inline">\(\widehat{\mathcal{Y}}
    \rightarrow  \mathcal{Y}\)</span> we wouldn’t be able to use <span
    class="math inline">\(f: \mathcal{X}\times \Theta \to
    \widehat{\mathcal{Y}}\)</span> as a label-predictor.</p></li>
    <li><p>For the regression problem, with <span
    class="math inline">\(\mathcal{Y}= \mathbb{R}^{\text{out}}\)</span>
    it is common to take the <span
    class="math inline">\(L^2\)</span>-norm as the cost function <span
    class="math inline">\(c(\hat y, y) = \|\hat y -
    y\|^2\)</span>.</p></li>
    <li><p>The parameter space <span
    class="math inline">\(\Theta\)</span> needs to be a differentiable
    manifold and <span class="math inline">\(\ell\)</span> needs to be a
    differentiable function of <span
    class="math inline">\(\mathbf{\boldsymbol{\theta}}\)</span> (at
    generic points) in order for the gradient methods to make
    sense.</p></li>
    <li><p>The most ubiquitous setting is when <span
    class="math inline">\(\Theta = \mathbb{R}^P\)</span> and we then use
    the gradient descent method <span
    class="math inline">\(\mathbf{\boldsymbol{\theta}}^{\text{updated}}
    = \mathbf{\boldsymbol{\theta}} - \alpha \nabla
    \ell(\mathbf{\boldsymbol{\theta}})\)</span>.</p></li>
    <li><p>On a <em>not necessarily flat</em> manifold, we would need a
    retraction function <span class="math display">\[\qquad \qquad
    r_{\mathbf{\boldsymbol{\theta}}} : T_{\mathbf{\boldsymbol{\theta}}}
    \Theta \to \Theta  \text{ satisfying }
    r_{\mathbf{\boldsymbol{\theta}}}(\mathbf{\boldsymbol{0}}) =
    \mathbf{\boldsymbol{\theta}} \text{ and }
    \frac{\,\mathrm{d}r_{\mathbf{\boldsymbol{\theta}}}(t
    \mathbf{\boldsymbol{v}})} {\,\mathrm{d}t}\bigg|_{t = 0} =
    \mathbf{\boldsymbol{v}},\]</span> or a partial retraction function
    <span class="math inline">\(r_{\mathbf{\boldsymbol{\theta}}}: U \to
    \Theta\)</span> for a neighborhood <span
    class="math inline">\(\mathbf{\boldsymbol{0}} \in U \subseteq
    T_{\mathbf{\boldsymbol{\theta}}}\Theta\)</span>. The raison d’être
    such a retraction is to be able to move on <span
    class="math inline">\(\Theta\)</span> starting from <span
    class="math inline">\(\mathbf{\boldsymbol{\theta}}\)</span>, in the
    direction of <span class="math inline">\(\mathbf{\boldsymbol{v}}\in
    T_\mathbf{\boldsymbol{\theta}} \Theta\)</span> and we update the
    values of the parameters by <span
    class="math inline">\(\mathbf{\boldsymbol{\theta}}^{\text{updated}}
    := r_\mathbf{\boldsymbol{\theta}}(- \alpha \nabla
    \ell(\mathbf{\boldsymbol{\theta}}))\)</span>, valid for small enough
    step size <span class="math inline">\(\alpha&gt;0\)</span> in
    partial retractions. For <span class="math inline">\(\Theta\)</span>
    an open domain in a flat affine space, <span
    class="math inline">\(r_{\mathbf{\boldsymbol{\theta}}}
    (\mathbf{\boldsymbol{v}}) = \mathbf{\boldsymbol{v}} +
    \mathbf{\boldsymbol{\theta}}\)</span> does the job, and gives us the
    gradient descent for <span
    class="math inline">\(\mathbf{\boldsymbol{v}} = -\alpha
    \nabla\ell(\mathbf{\boldsymbol{\theta}})\)</span>.</p>
    <figure>
    <img src="./figures/retraction.svg" style="width:70.0%" />
    <figcaption>Gradient descent on the parameter manifold. The dotted
    path <span class="math inline">\(\{
    r_{\mathbf{\boldsymbol{\theta}}}(t\mathbf{\boldsymbol{v}}): t \in
    [0,1]\}\)</span> on the manifold is produced via the retraction.
    Starts off aligned with <span
    class="math inline">\(\mathbf{\boldsymbol{v}}\)</span> when <span
    class="math inline">\(t\)</span> is infinitesimally small, and the
    retraction makes sure that the path stays on the manifold for
    nonzero finite <span class="math inline">\(t\)</span>.</figcaption>
    </figure></li>
    <li><p>The data space <span
    class="math inline">\(\mathcal{X}\)</span> does not technically need
    any structure more than a set, however in most circumstances it will
    be a differentiable manifold—in fact simply <span
    class="math inline">\(\mathcal{X}=
    \mathbb{R}^{\text{in}}\)</span>—and in the case of neural networks,
    <span class="math inline">\(f(\mathbf{\boldsymbol{x}},
    \mathbf{\boldsymbol{\theta}})\)</span> is a differentiable function
    of <span class="math inline">\(\mathbf{\boldsymbol{x}}\)</span>,
    using which we can compute saliency of features.</p></li>
    <li><p>In case of <span class="math inline">\(\Theta =
    \mathbb{R}^P\)</span>, we have <span
    class="math inline">\(T_\mathbf{\boldsymbol{\theta}} \Theta \cong
    \mathbb{R}^P\)</span> for all points <span
    class="math inline">\(\mathbf{\boldsymbol{\theta}}\)</span> on the
    manifold, and in fact they can be all canonically identified with
    one another. That is why we can assume all the gradients <span
    class="math inline">\(\nabla
    \ell(\mathbf{\boldsymbol{\theta}})\)</span> are living in the same
    vector space, and why it is meaningful to accumulate gradients from
    past steps such as in momentum.</p></li>
    <li><p>If furthermore the the tangent spaces <span
    class="math inline">\(T_{\mathbf{\boldsymbol{\theta}}}\Theta\)</span>
    have distinguished bases—for example the directions aligned with
    weights is distinguished from the point of view of the model
    architecture—then fixing such a basis one can also use componentwise
    operations on the gradient vectors such as the squaring, division,
    taking squareroots etc. used in Adam.</p></li>
    </ul>
  </main>

  <!-- Bottom navigation (optional) -->
  <nav class="post-nav-bottom">
        <a href="blog-index.html">← Back to Blog Index</a>
      </nav>

  <!-- Author and date at bottom -->
    <footer class="post-footer">
    <div class="author-date">
      <p class="author">emk</p>
      <p class="date">2025/06/05</p>
    </div>
  </footer>
  

</body>
</html>